# -*- coding: utf-8 -*-
"""
Created on Fri Nov  9 18:19:43 2018

@author: matta
"""

#importation des librairies
import numpy as np
from pylab import *
import random 
atchoum = random.randrange(20,50)
print(atchoum)
LEARNING_RATE = 0.000005
#création des variables
exected_output = [0., 1., 1., 0.]
layer_1 = [[0., 0., 1., 1.],
           [0., 1., 0., 1.]]

bias_layer_1 = [-10., -30]
bias_layer_2 = [-20.]

weight_layer_1 = array([[20., 20.],
                        [20., 20.]])
weight_layer_2 = array([[atchoum, -300.]])
print(weight_layer_2[0,0])

#création des fonctions
def sigmoid(z):
    return 1.0/(1.0+np.exp(-z))

def relu(x):
    return x * (x > 0)
    
def calculate_pd_e1_output(actual_output, target_output):
    return actual_output - target_output

def calculate_pd_out_net(out):
    return 1 * (out > 0)#out*(1-out)

def calculate_pd_net_w(out):
    return out

def remove_bias(matrice, bias):
    """
    Input : matrix, vector of bias
    Output : matrix
    Do : remove the umpteenth bias to the umpteenth ligne 
    of the matrix
    """
    len_b = len(bias) #calcul la longeur de la liste des bias
    for i in range(len_b):
        matrice[i,:] += bias[i] #enleve le iéme bias à la ligne i de la matrice
    return matrice

def costa(out, target):
    len_target = len(target)
    cost = 0
    for i in range(len_target):
        cost += (target[i] - out[0, i])**2
    return 1/len_target * cost

epsi = []
#corp programme
for i in range(100):
    layer_2 = relu(remove_bias(dot(weight_layer_1, layer_1), bias_layer_1))
    print(layer_2)
    layer_3 = relu(remove_bias(dot(weight_layer_2, layer_2), bias_layer_2))
    print(layer_3)
    cost = costa(layer_3, exected_output)
    print("cost",cost)
    epsi.append(LEARNING_RATE * calculate_pd_e1_output(layer_3[0,2], exected_output[2]) * calculate_pd_out_net(layer_3[0,2]) * calculate_pd_net_w(layer_3[0,2]))
    print("epsi", epsi[i])
    weight_layer_2[0,0] -= epsi[i]
    print(weight_layer_2[0,0])

plot(epsi)
